<!doctype html>
<html lang="en">
<head>
<title>Reinforcement Learning for Sensory Inputs</title>
<meta property="og:title" content=Reinforcement Learning for Sensory Inputs" />
<meta name="twitter:title" content="Reinforcement Learning for Sensory Inputs" />
<meta name="description" content="An Analysis of Playing Atari with Deep Reinforcement Learning." />
<meta property="og:description" content="An Analysis of Playing Atari with Deep Reinforcement Learning." />
<meta name="twitter:description" content="An Analysis of Playing Atari with Deep Reinforcement Learning." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">Playing Atari With Deep Reinforcement Learning</nobr>
 <nobr class="widenobr">For CS 7150</nobr>
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>An Analysis of Playing Atari With Deep Reinforcement Learning</h2>
<p>Revolutionizing the field of Reinforcement Learning by combining existing methods with Deep Learning networks on raw sensory input.</p>
</div>
</div>
<div class="row">
<div class="col">

<h2>Paper Overview</h2>
<p>The 2013 research paper "Playing Atari with Deep Reinforcement Learning" by Volodymyr Mnih and colleagues at DeepMind Technologies introduces a groundbreaking deep learning model. This model, a convolutional neural network, uses reinforcement learning to learn control policies directly from high-dimensional sensory inputs, specifically raw pixels from Atari 2600 games. The significance of this work lies in its demonstration of an AI system learning and performing at or above human level without needing manual feature extraction, representing a major advancement in AI capabilities.</p>

<h2>Literature Review</h2>
<p>The development of deep reinforcement learning, as showcased in this paper, is built upon earlier significant works. Pioneering algorithms like TD-gammon by Gerald Tesauro and Neural Fitted Q-learning (NFQ) by Martin Riedmiller laid the groundwork for this field. Additionally, the Arcade Learning Environment, developed by Marc Bellemare and others, provided a standard testing platform for reinforcement learning algorithms on Atari 2600 games. These foundational works were crucial for the advancements made in "Playing Atari with Deep Reinforcement Learning."</p>

<h2>Methodology and Technical Details</h2>
<p>The cornerstone of the research presented in "Playing Atari with Deep Reinforcement Learning" is its innovative network architecture. The model is a convolutional neural network (CNN), designed specifically to process and interpret the visual input from Atari games.</p>
<img src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnature14236/MediaObjects/41586_2015_BFnature14236_Fig1_HTML.jpg?as=webp" alt="Network Architecture Diagram" style="display: block; margin: auto;">
<p>The architecture comprises three convolutional layers and two fully connected layers. The network processes input in the form of an 84x84x4 image, which is derived from preprocessing the raw frames of Atari 2600 games. Here's a breakdown of the layers:</p>
<ul>
    <li><strong>First Hidden Layer:</strong> This layer applies 16 8x8 filters with a stride of 4 to the input image. Following the convolution, a rectifier nonlinearity is applied.</li>
    <li><strong>Second Hidden Layer:</strong> This layer consists of 32 4x4 filters with a stride of 2, again followed by a rectifier nonlinearity.</li>
    <li><strong>Third Hidden Layer:</strong> The final hidden layer is fully connected, containing 256 rectifier units.</li>
    <li><strong>Output Layer:</strong> The output is a fully-connected linear layer, corresponding to each valid action in the game. The number of valid actions varies between 4 and 18, depending on the game.</li>
</ul>

<h2>Biography</h2>
<p>The paper's authors, all members of DeepMind Technologies at the time of writing, bring diverse and rich academic and professional backgrounds. Volodymyr Mnih, the lead author, holds a PhD in Machine Learning from the University of Toronto. His colleagues include Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller, each bringing unique expertise and experience from various prestigious institutions and fields of study.</p>

<h2>Social Impact</h2>
<p>The method presented in the paper has far-reaching implications for society. Positively, it opens avenues in robotics, autonomous vehicles, and advanced gaming systems. However, it also raises concerns about its potential use in autonomous weapons systems and gambling, which could have detrimental societal impacts. The ability of these models to learn and operate autonomously in complex environments is both an opportunity and a challenge for ethical considerations in technology development.</p>

<h2>Industry Applications</h2>
<p>The method detailed in this paper has practical applications in the development of AI and robotics, particularly in systems that require autonomous decision-making based on sensory inputs. Its application in self-driving cars is a notable example, showcasing its potential in enhancing decision-making processes in autonomous systems.</p>

<h2>Follow-on Research</h2>
<p>The paper lays the foundation for further research in applying deep reinforcement learning to more complex and contemporary games. This progression from Atari games to more sophisticated gaming environments can provide deeper insights and more robust applications of AI in various fields.</p>

<h2>Peer-Review</h2>

<p>Just as we have done in the role-playing exercise, analyze the paper from all perspectives.
</p>

<h2>References</h2>
<ul>
    <li><a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Mnih, V., et al. (2013). <em>Playing Atari with Deep Reinforcement Learning</em>. NIPS Deep Learning Workshop, 2013.</a></li>
    <li><a href="https://www.csd.uwo.ca/~xling/cs346a/extra/tdgammon.pdf">Tesauro, G. (1995). <em>TD-Gammon: A Self-Teaching Backgammon Program</em>. Semantic Scholar.</a></li>
    <li><a href="https://www.semanticscholar.org/paper/Neural-Fitted-Q-Iteration-First-Experiences-with-a-Riedmiller/282001869bd502c7917db8b32b75593addfbbc68">Riedmiller, M. (2005). <em>Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method</em>. Semantic Scholar.</a></li>
    <li><a href="https://ar5iv.org/abs/1207.4708">Bellemare, M., et al. <em>The Arcade Learning Environment: An Evaluation Platform for General Agents</em>. Ar5iv.</a></li>
</ul>

<h2>Team Members</h2>                                       
<p>Chase Kenyon, Nicholas Calvaresi.</p>

  
</div><!--col-->
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://cs7150.baulab.info/">About CS 7150</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
